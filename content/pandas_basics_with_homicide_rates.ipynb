{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28c016f-d618-48db-9787-ccd2df6e1690",
   "metadata": {},
   "source": [
    "# Wrangling and Analyzing Homicide Data with Pandas\n",
    "\n",
    "Below we'll use some fake data -- generated by ChatGPT for learning purposes -- on city populations and murder counts to practice basic data wrangling and analysis skills using the **pandas** library.\n",
    "\n",
    "The skills we'll cover include:\n",
    "\n",
    "- Creating a DataFrame\n",
    "- Slicing and Dicing DataFrames (aka [indexing](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing)).\n",
    "- Adding new columns based on a calculation\n",
    "- Merging DataFrames\n",
    "- Aggregating data\n",
    "- Sorting and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0021fc-3c5a-4d83-a35f-260a71711981",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose']\n",
    "population = [3926219, 6411233, 3272371, 3003751, 6621623, 1609186, 2705762, 2235335, 5075959, 4356045]\n",
    "murders_by_city = [219, 476, 127, 239, 220, 497, 498, 413, 311, 398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f58913-e3c9-4623-a650-9e4320832b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c8737-75fe-4c33-adac-b26ef91887d8",
   "metadata": {},
   "source": [
    "Let's assume that our population data comes from one source (e.g. the Census) and our data on homicides comes from another (e.g. the FBI UCR crime stats).\n",
    "\n",
    "Below, we'll create two DataFrames using our fabricated data: one containing the population of each city, and the other the count of murders. \n",
    "\n",
    "You'll notice that we're using the `cities` list in both DataFrames. The city name will serve as a unique identifier common to both data sets, which is important when it comes time to join the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab93a89-9245-4e8d-aab2-fa6abaeb2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = pd.DataFrame({'cities': cities, 'pop': population})\n",
    "murders = pd.DataFrame({'cities': cities, 'murders': murders_by_city})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31b56b-e2ce-4844-81ce-fe9975a25186",
   "metadata": {},
   "source": [
    "Now let's merge those DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5a3dd-54fe-46a3-8924-37bd160a843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pops.merge(murders, on='cities', how='inner')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b4a2d-094f-461d-bec2-e7be0067cf34",
   "metadata": {},
   "source": [
    "**Task**: Verify that the join didn't cause records to be dropped by comparing the size of the original DataFrames with the merged DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d061e-07a3-4984-9643-251994a32ccc",
   "metadata": {},
   "source": [
    "## Slicing and dicing a DataFrame\n",
    "\n",
    "There are a head-spinning variety of ways to use row position, column names, or a combination of both to select a subset of data in a DataFrame. \n",
    "\n",
    "While you may not use all of these techniques on a regular basis, you should at least be aware of them since you'll see them in other people's code. \n",
    "\n",
    "Below is a brief overview of some of the more common techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e1343-32a9-408f-8907-09e97404d4b7",
   "metadata": {},
   "source": [
    "### `.loc`\n",
    "\n",
    "Select a single value by index (aka row) and column. \n",
    "\n",
    "> Note that start counting at 0 and we use square brackets (NOT parentheses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05cd83-5dc0-4c15-b6f1-89fba0b97a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'cities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a1639-c5b2-451d-bd0d-1534ff419848",
   "metadata": {},
   "source": [
    "Select a range of rows and column.\n",
    "\n",
    "> Note, unlike Python lists, the end position is \"inclusive\". In other words, the item will be included in the list returned by the range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead96161-c3cd-4ec6-8631-57173999c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:1, 'cities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fabaa7-cf74-4f22-aff0-f49327f9aa6a",
   "metadata": {},
   "source": [
    "Select range of rows and multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87bfbb-2e0f-4fc7-ae4f-740a7f153bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2:5, ['cities', 'pop']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c945474-9a05-4111-bdc0-f355cbc4cce3",
   "metadata": {},
   "source": [
    "Select range of rows and **all** columns.\n",
    "\n",
    "> Note the `:` tells `loc` to include all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c20ed-c21c-43a2-92f2-83a72237c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada0a8d-f89e-4df1-a418-c381ac2d9bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b243c455-878a-4ff9-abfe-a8d9afccba11",
   "metadata": {},
   "source": [
    "### `.iloc`\n",
    "\n",
    "The `.iloc` method on a DataFrame can be used to grab entire, including all columns, simply using the a range of the row index positions. This is similar to plain old `.loc`, but you don't need to specify a second argument for columns.\n",
    "\n",
    "The `i` in `iloc` refers to index, or row number, starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf430fa-83b7-4153-adfc-da9f0f2f0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05384f9-0c2e-4d56-9eb0-e0465eedf7fe",
   "metadata": {},
   "source": [
    "### Square brackets selection\n",
    "\n",
    "Yet another way to select rows, with arguably the simplest syntax, is to simply use the square brackets `[]` on a DataFrame, similar to how we filter the DataFrame.\n",
    "\n",
    "> Note that iunlike `.loc` and `.iloc`, the second number (`2`) is **not** inclusive, so we only get the results in rows 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c3fdd-132e-47a4-8a44-16fd3e826705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9ba3-ae9c-48a2-9f68-43c198aa601f",
   "metadata": {},
   "source": [
    "## Adding new columns\n",
    "\n",
    "It's often handy to apply functions to a column, for example to generate a new column. To do so, we use the `apply` method on a particular column.\n",
    "\n",
    "Below, we apply Python's built-in `str.upper` method to all values in the `cities` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0c276-9d68-403a-b5e1-6e6fffa70c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cities.apply(str.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adf90d-5b59-4911-a85b-19a54c7dc60f",
   "metadata": {},
   "source": [
    "Note that the original values haven't changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23707f7-d763-44d0-b709-dbd2d294c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87986fa7-e2b2-4b23-86be-6a6632bd4e96",
   "metadata": {},
   "source": [
    "To retain those upper-cased city names, we can assign the results of the `apply` function to a new column using the square brackets syntax:`df['name_of_new_field']`.\n",
    "\n",
    "Take special note that we're **not** actually calling the function/method or method that is passed to `apply`. You can tell because we're not ending the name with parentheses containing an argument. Normally, we'd \"call\" the function or method as so:\n",
    "\n",
    "```python\n",
    "# Use parens to call \"upper\" and pass in a string, \n",
    "# since that's what this method expects\n",
    "\n",
    "str.upper(\"this is all lower case\")\n",
    "```\n",
    "\n",
    "We're simply passing `str.upper` to `apply`, which in turn...well...*applies* it to each value in the column. The `apply` method is actually calling the `str.upper` method for us behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a1876-384e-4522-8e7a-b7392680c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cities_upper'] = df.cities.apply(str.upper)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4635c-9e65-463e-864b-8e4da03504e5",
   "metadata": {},
   "source": [
    "### Sidenote\n",
    "\n",
    "We might apply other clean-ups or standardizations to such a column in the real world, to ensure that the values are identical in both data sets. This is important when we want to join two datasets, which requires each dataset to contain one or more columns that can be used to match rows between them. \n",
    "\n",
    "*Ensuring consistency in the values we're using to join datasets is critical to avoid losing records during the merge process.*\n",
    "\n",
    "In this simple exercise, that shouldn't be a problem because we'll use the same list of `cities` (created at the top of this notebook) in both DataFrames.\n",
    "\n",
    "### Dropping a note\n",
    "\n",
    "So let's just remove the new `cities_upper` column to keep our dataframe tidy.\n",
    "\n",
    "> Note that we have to specify the `axis=1` argument. This tells the DataFrame to search for the label by column, rather than by index (ie row), which is the default behavior. We also use the `inplace=True` argument to modify the original dataframe, rather than returning a copy without the `cities_upper` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4186b77-33e9-429b-88b5-c6368f9b57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('cities_upper', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d7a39-5b8d-469f-97ca-30cfe9e927b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981d94b-55f9-49f6-9a31-40701c368125",
   "metadata": {},
   "source": [
    "## Calculating a per capita rate\n",
    "\n",
    "Now that we're comfortable creating news columns, let's try calculating the murder rate for this data. We'll pretend we're working with data from 2023, so we'll call the new field  `rate23`. This will help us differentiate this rate from another identical calculation later on, when we merge data from a prior year.\n",
    "\n",
    "You can once again use `apply` to calculate a rate for each row. \n",
    "\n",
    "Due to the low number of murders relative to the overall population, we'll use a larger \"multiplier\" in our calculation (100,000 instead of 1,000). That should produce a more human-friendly number (typically above 1) as a result.\n",
    "\n",
    "Below is one way to perform the calculation. \n",
    "\n",
    "A few important items to note:\n",
    "\n",
    "- The `axis=1` argument, which tells pandas to apply the calculation to each row\n",
    "- The use of a [lambda](https://docs.python.org/3/reference/expressions.html), which is a way of writing an anonymous, inline function -- i.e. one without a name using the traditional `def my_function_name():` syntax. Lambdas are handy for short snippets of code. If the logic or math was significantly more complex, we might want to create a custom function and then pass that into `apply` instead.\n",
    "- It might be hard to tell from the lambda defintion, but once again, we're not actually calling the lambda function. `apply` is doing that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd62a16-59df-4528-a43d-a426eb3ef5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda row: (row['murders'] / row['pop']) * 100_000, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae17af-4320-4951-8c2e-72a9f261d284",
   "metadata": {},
   "source": [
    "The above works, but there's a simpler and faster way to generate this new column that is especially well-suited to short bits of math/logic. You can simply select the columns from the DataFrame and use standard math operators such as division and multiplication. This can have several benefits:\n",
    "\n",
    "- The code is more *readable*.\n",
    "- The operation is more *efficient* or fast because, under the hood, the DataFrame is using a technique called \"vector math\", rather than applying a custom function to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbe654-de64-494c-8c86-5bf0a469283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['murders'] / df['pop'] * 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1271d-6beb-4232-a9c3-d28ae2b0620c",
   "metadata": {},
   "source": [
    "The above appears to generate the same answers as our `apply` approach earlier, so let's go ahead and re-run the calculation, this time storing it in a new DataFrame column called `rate23`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cad14-1aca-4d6d-9022-52368ddef623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rate23'] = df['murders'] / df['pop'] * 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88fe509-7052-4e57-b665-f23b9fb94ab3",
   "metadata": {},
   "source": [
    "## Sorting by rate\n",
    "\n",
    "At this point, we can use some simple sorting to find the city with the highest murder rates in 2023: Philadelphia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f9316-f832-445d-86ea-404c87ac2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('rate23', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682b350-5f8a-48d4-b82b-444132364a92",
   "metadata": {},
   "source": [
    "## Filtering and sorting\n",
    "\n",
    "And if we can add some filtering to find the cities with murder rates greater than or equal to 10 per 100,000 people\n",
    "\n",
    "> Note, we'll filter first, then apply sorting. You could do this in reverse (ie sort first), but the sorting operation will be quicker on a smaller data set than on the entire data set. The difference in speed becomes much more noticeable as datasets increase in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41679c-eb3d-4406-8c08-1a0623fe3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a filter for our DataFrame first\n",
    "rate_filter = df.rate23 >= 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8a471-6ff3-4ada-8716-e88a088992a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then apply the filter, followed by the sorting\n",
    "df[rate_filter].sort_values('rate23', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99008e2b-eb6f-4a22-b712-85fd44d4db2f",
   "metadata": {},
   "source": [
    "## Adding another year to the mix\n",
    "\n",
    "Let's say we have additional data for the year 2022. *Again, this is totally fake population and homicide data generated for the purposes of the exercise.*\n",
    "\n",
    "We can rinse and repeat the above steps to calculate the homicide rate for 2022.\n",
    "\n",
    "We'll reuse the original `cities` variable defined at the beginning of our notebook, and simply add new population and homicide numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48035a23-5680-4a5b-9271-31d9fc49d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop2 = [3732063, 6334621, 3759166, 2825857, 6936845, 1690307, 3111121, 1956014, 5423644, 4561616]\n",
    "murders2 = [284, 536, 36, 240, 201, 553, 539, 503, 232, 363]\n",
    "\n",
    "df2 = pd.DataFrame({'cities': cities, 'pop': pop2, 'murders': murders2})       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d181d11-61a7-4868-a048-d42417e1a6c1",
   "metadata": {},
   "source": [
    "Now we're ready to calculate the murder rate for 2022 using the new DataFrame `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e7824-17b2-44f1-a54d-ab3e769e6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['rate22'] = df2['murders'] / df2['pop'] * 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901628a-f2e1-4eef-8b4a-10b7ac7528f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b11814-b246-4033-b20f-e73269460d6d",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Repeat the basic analyses we performed on the original 2023 data. Specifically\n",
    " \n",
    "- Sort the 2022 DataFrame to fin the city with the highest murder rate\n",
    "- Filter *and* sort the DataFRame to find cities with a murder rate greater than or equal to 10 per 100,000 people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6d2cf-715b-4428-b873-f94977ccf697",
   "metadata": {},
   "source": [
    "## Calculating the rate of change over years\n",
    "\n",
    "We now have two dataframes showing the murder rates for these cities in 2022 and 2023. \n",
    "\n",
    "To find which cities had the largest increase or decrease in homicide rates over these years, we can merge these datasets and perform a rate change calculation.\n",
    "\n",
    "\n",
    "### Joining data\n",
    "\n",
    "Step 1 is joining these DataFrames on a common \"key\" or column -- in this case `cities`.\n",
    "\n",
    "Note that because we have columns with identical names across these data sets, pandas automatically prefixes them with an `x_` and `y_` to allow us to tell them apart. \n",
    "\n",
    "The `x_` is assigned to columns from the DataFrame on the left side of the `.merge` method (ie the DataFrame that we're merging into); the `y_` is applied to the DataFrame passed into the `.merge` method (ie the one that is being merged in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b3fe5-024a-4138-a694-9b756283c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.merge(df2, on='cities')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee6d49-db3b-4a2e-90db-a543c1f58273",
   "metadata": {},
   "source": [
    "### Rate of Change\n",
    "\n",
    "After joining our data, we can add a new column containing the rate of change between 2022 and 2023.\n",
    "\n",
    "As a refresher, that calculation involves:\n",
    "\n",
    "- Subtracting the original rate from the new rate\n",
    "- Dividing the result by the original rate\n",
    "- Multiplying the result by 100 to produce a percent change\n",
    "\n",
    "In simple terms: `(New - Old / Old) * 100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3149e2-0f0d-48e6-a5c0-fe76169d2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['rate_change'] = (df_merged['rate23'] - df_merged['rate22'] ) / df_merged['rate22'] * 100\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a24eb-ee61-46ee-8941-5667c958b2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
