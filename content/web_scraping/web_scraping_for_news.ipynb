{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92be587",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Web Scraping for the News\n",
    "\n",
    "## What exactly *is* web scraping?\n",
    "\n",
    "\"Web scraping\" is an overloaded phrase.\n",
    "\n",
    "Depending on who you ask, web scraping could involve one or more of the following activities:\n",
    "\n",
    "- Downloading structured data used by a website to generate content (e.g. a JSON API that supplies data for a table).\n",
    "- Plucking data from the HTML of web pages\n",
    "- Gathering PDFs, audio, video, screenshots or other files hosted on websites\n",
    "- Extracting data from scraped PDFs\n",
    "- Transcribing scraped audio and video\n",
    "- Standardizing and adding new columns to scraped data\n",
    "- Storing data and files gathered from websites in databases, [\"buckets\" in the cloud](https://en.wikipedia.org/wiki/Object_storage), and other long-term storage locales for analysis, search or archiving.\n",
    "\n",
    "All of these activities are common in data and document processing pipelines that power news stories, interactive graphics and apps.\n",
    "\n",
    "But for our purposes, we're going to use a more narrow definition.\n",
    "\n",
    "<dl>\n",
    "    <dt><strong>web scraping</strong></dt>\n",
    "    <dd><em>The act of automating the acquisition of data or other files\n",
    "(images, videos, documents, etc.) from the web. The data may live on one or more pages of a website, or perhaps many different websites. At root, web scraping involves writing code to mimic the actions a human might take to visit a site in a web browser and manually gather files or data.</em></dd>\n",
    "\n",
    "## Value of scraping\n",
    "\n",
    "Journalistically valuable information is often locked up on a website that lacks easier methods for data acquisition. Not all government agencies, for example, offer downloadable [CSV files](https://en.wikipedia.org/wiki/Comma-separated_values) or [APIs](../apis/README.ipynb). Nor do they always respond to public records requests in a timely or helpful manner.\n",
    "\n",
    "**Web scraping allows journalists to acquire information in the face of technical or bureaucratic hurdles.**\n",
    "\n",
    "Scraping is also useful in scenarios where a website offers the most up-to-date or widest scope of information. In such cases, web scraping can help journalists tell a more accurate and timely story.\n",
    "\n",
    "Here are a few examples where web scraping helped produce news:\n",
    "\n",
    "* [Accidential shootings involving kids often go unpunished](https://apnews.com/article/32e2ce4e701f4448b3d9ba355edfa31d), by The Associated Press, relied on data scraped from the [Gun Violence Archive](https://www.gunviolencearchive.org/).\n",
    "* [Amazon Says It Puts Customers First. But Its Pricing Algorithm Doesn't](https://www.propublica.org/article/amazon-says-it-puts-customers-first-but-its-pricing-algorithm-doesnt), by ProPublica. Here's the [behind-the-scenes look](https://www.propublica.org/article/how-we-analyzed-amazons-shopping-algorithm) at how they scraped and analyzed Amazon data.\n",
    "* [Dollars for Docs](https://projects.propublica.org/docdollars), a searchable news app by ProPublica. Here's a write-up on the [scraping aspect](https://www.propublica.org/nerds/scraping-websites) of the work.\n",
    "\n",
    "\n",
    "## Scraping - The Technical Bits\n",
    "    \n",
    "Every web site has its own [personality](website_personalities.ipynb).\n",
    "    \n",
    "Head to [Web Scraping 101](scraping_101.ipynb) to learn how to scrape different types of sites, from simple to challenging.\n",
    "\n",
    "## The option of last resort\n",
    "\n",
    "Web scraping is a brittle activity. Sites move, URL\n",
    "and page structures evolve, interactivity gets added or removed.\n",
    "\n",
    "Shiny new web scrapers inevitably break in the days, months and years after they were written.\n",
    "\n",
    "And often, websites do *not* reflect the most recent or most accurate information.\n",
    "\n",
    "**For these reasons, scraping should be treated as an option of last resort.**\n",
    "    \n",
    "When a government website does not offer easy methods for obtaining data, journalists typically reach out to the agency and possibly file public records requests to obtain structured data or digital files. They seek to exhaust easier options before turning to their scraping toolkit. \n",
    "\n",
    "## Ethical scraping\n",
    "\n",
    "Scraping ethically implies a number of best practices. To mention a few:\n",
    "\n",
    "* Respecting a site's terms of use\n",
    "* Identifying yourself clearly\n",
    "* Taking care not to overwhelm a site with large volumes of requests\n",
    "\n",
    "Here are a few articles that lay out ethical concerns in more detail:\n",
    "\n",
    "* [On the Ethics of Web Scraping and Data Journalism](https://gijn.org/stories/on-the-ethics-of-web-scraping-and-data-journalism/)\n",
    "* [Ethics in Web Scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n",
    "\n",
    "Keep in mind that opinions vary about what is or is not \"ethical\" -- or legal -- when it comes to scraping. It's an issue that [has been tested in the courts](https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data) and will continue to be fought over.\n",
    "\n",
    "*Be mindful of your legal responsibilities and potential liability when scraping the web.* If in doubt, contact a legal organization that provides advice to journalists, such as our friends at the [First Amendment Coalition](https://firstamendmentcoalition.org/).\n",
    "\n",
    "## Let's scrape something already\n",
    "\n",
    "Sheesh. That was a lot of preamble.\n",
    "    \n",
    "Let's write some code.\n",
    "    \n",
    "Below is an extremely simple example of web scraping. It's designed to whet your appetite. We'll wrestle with more challenging sites down the road.\n",
    "    \n",
    "In the below example, we use a pair of Python libraries to:\n",
    "    \n",
    "- Fetch the HTML of the home page of <http://example.com> (the [requests][] bit)\n",
    "- Extract the page's title from the HTML (the [BeautifulSoup][] bit)\n",
    "    \n",
    "Enjoy this amuse bouche. It's about as easy as scraping will ever get.\n",
    "\n",
    "[requests]: https://docs.python-requests.org/en/latest/index.html\n",
    "[BeautifulSoup]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "> If you're working in GitHub Codespaces, we've installed these libraries for you. If you're working locally, `pip install requests bs4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, requests\n",
    "url = \"http://www.example.com\"\n",
    "html = requests.get(url).text\n",
    "soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "h1 = soup.find('h1')\n",
    "print(h1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd6e89",
   "metadata": {},
   "source": [
    "> If you're curious why we include `html.parser` as an option when we call [BeautifulSoup][],\n",
    "> check out bs4's docs on [installing a parser][] and [differences between parsers][].\n",
    "\n",
    "[BeautifulSoup]: https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup\n",
    "[installing a parser]: https://beautiful-soup-4.readthedocs.io/en/latest/#installing-a-parser\n",
    "[differences between parsers]: https://beautiful-soup-4.readthedocs.io/en/latest/#differences-between-parsers\n",
    "\n",
    "\n",
    "## What's Next\n",
    "\n",
    "- [Web Scraping 101](scraping_101.ipynb) \n",
    "- [Dissecting Websites](dissecting_websites.ipynb)\n",
    "- [Website Personalities](website_personalities.ipynb) - Common website traits (aka scraping challenges) and how to address them\n",
    "  - [Skip the Scraping. Cheat](skip_scraping_cheat.ipynb) - You'll thank us\n",
    "  - [WYSIWYG Scraping](wysiwyg_scraping.ipynb) - Old school, easy scrapes. We miss you, Internet 2005\n",
    "  - [Drive the Browser, Robot](drive_the_browser_robot.ipynb) - Sometimes it's easier to have the machines do the work\n",
    "- [Scraping exercises](exercises.ipynb) - A few sites to challenge your scraping skills.\n",
    "- [Scraping resources](resources.ipynb) - Tutorials, key concepts, code libraries for scraping, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d6e40-1510-4d0d-bcca-e06e0417c390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
