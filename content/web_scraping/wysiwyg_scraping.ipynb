{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "370a2fe7-649a-49ad-a6c0-f39e05f00ccf",
      "cell_type": "markdown",
      "source": "# WYSIWYG Scraping\n\nThere's no arguing that nerds love acronyms. And one of the best acronyms, imho, is [WYSIWYG](https://en.wikipedia.org/wiki/WYSIWYG) (pronounced wiz-ee-wig): *What You See Is What You Get*.\n\nOriginally, WYSIWYG referred to software that \"[allows users to see and edit content in a form that appears as it would when displayed on an interface, webpage, slide presentation or printed document.\"](https://www.techtarget.com/whatis/definition/WYSIWYG-what-you-see-is-what-you-get)\n\nThat was a sea change from having to, for example, edit HTML directly. Instead, you could hit a button on an editing panel to make some text **bold**, and the text would display in an editing window as **bold** (as opposed to displaying raw HTML such as `<strong>bold</strong>`).\n\nWhy bring this up?\n\nBecause it's a useful analogy for web pages. Back in the days of yore, many (perhaps most?) websites followed the WYSIWYG principle. These were simpler times, when the content displayed on a web page closely matched the HTML in the underlying document for a page.\n\nIf your web browser showed a table of data, it was quite likely that you'd find a `<table>` element somewhere in the page's HTML. \n\nThis made web scrapers happy.\n\nWe could write simple code to grab the page, pluck out the data, and be on our way.\n\n## A WYSIWYG example.com\n\nThose halcyon [days are waning](drive_the_browser_robot.ipynb), but you can still find websites in the wild that are coded this way.\n\nFor example, compare the browser display of <http://example.com> and its underyling HTML.\n\n![example dot com display vs HTML](../files/example_dot_com_wysiwyg.png)",
      "metadata": {}
    },
    {
      "id": "9926de12-4eed-45ed-af31-4ff03d3c8915",
      "cell_type": "markdown",
      "source": "## Failed Banks Simple Scrape\n\nSimilar to <http://example.com>, the [FDIC's list of failed banks](https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/) also embeds its content -- at least the data we care about -- in the HTML source code. The data on the site is \"paged\" -- meaning that it only displays the first 10 banks by default. To view or scrape all 500-plus banks, you would need to step through each of the pages to gather all the content. For this tutorial, we'll focus on the simpler task of scraping just the 10 banks on the home page, and leave page-handling as an exercise at the end of this tutorial.\n\n> IMPORTANT: The site also provides the data as a downloadable CSV, which is a better alternative to scraping. But we'll ignore that for now and use the page as an opportunity to practice basic scraping techniques.\n\nHow do we know that the HTML source code contains the bank data?\n\nBy using the browser's developer tools to [dissect the website](dissecting_websites.ipynb).\n\n## View Page Source\n\nStep 1 involves right clicking on the page and selecting `View Page Source`.\n\n> *Note: The precise wording of the menu options may vary by browser. We recommend Firefox or Chrome, by the way.*\n\n![FDIC view page source](../files/fdic_banks_html.png)",
      "metadata": {}
    },
    {
      "id": "eed6f918-2a6f-409d-a925-58fefe9f4610",
      "cell_type": "markdown",
      "source": "## Compare What You See to Page Source\n\nThe next step involves some basic code sleuthing: \n\n- Review the list of banks as displayed in the browser. For example, we can see above ☝️that  `Pulaski Savings Bank` in Illinois is at the top of the list.\n- Now head over to the HTML source page you just opened (using `View Page Source`) and search the page for `Pulaski Savings Bank`.\n\nYou should see that the data is embedded in the HTML.\n\n![FDIC html source code](../files/fdic_view_page_source.png)",
      "metadata": {}
    },
    {
      "id": "5b871cc8-5f8a-42e5-b580-a72c8954d93a",
      "cell_type": "markdown",
      "source": "## Get ready to scrape\n\nThis simple page allows us to use two of the traditional workhorses of Python web scraping: the [requests](https://requests.readthedocs.io/en/latest/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) libraries.\n\nThe former gives us the ability to fetch the source HTML for the page. The latter provides a Pythonic \"interface\" to the HTML that lets us easily extract data points.",
      "metadata": {}
    },
    {
      "id": "951fd895-aed5-4a87-ab0e-ebecc9f31abb",
      "cell_type": "markdown",
      "source": "## Fetch the page\n\n> **IMPORTANT**: This section of the tutorial using `requests` only works on GitHub Codespaces or locally (if you've cloned this repo to your own machine).\n\nLet's start our scrape by requesting the page content.",
      "metadata": {}
    },
    {
      "id": "a9b2131f-3e25-49ec-8d93-603f25f5f7a2",
      "cell_type": "code",
      "source": "import requests\nurl = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\nresponse = requests.get(url)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e753fa48-69a7-4508-943c-ec135a99d345",
      "cell_type": "markdown",
      "source": "Now inspect the `response` to see what we're working with.",
      "metadata": {}
    },
    {
      "id": "77d9f7c5-2cc5-4858-a8db-c01b5ed01007",
      "cell_type": "code",
      "source": "# Get the type\ntype(response)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ecc841d9-b99b-42f0-a312-0e5a8d2faaad",
      "cell_type": "code",
      "source": "# Ouch, that's a long list!\ndir(response)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "96655015-6ea9-44ca-86a2-1989c5e648ac",
      "cell_type": "markdown",
      "source": "If you review the `response` object's attributes (see the [Hidden Life of Objects](../classes_and_oop/hidden_life_of_objects.ipynb) for background), you'll notice there are some potentially handy bits of data and functionality. \n\n> It's not a bad idea to review the [requests documentation on responses](https://requests.readthedocs.io/en/latest/user/quickstart/#response-content) as well. \n\nFor our purposes, we're going to grab the `text` of the response, which should contain the raw HTML of the page.\n\nPrepare yourself for a LOT of HTML.\n\n> BTW, you can clear the output by selecting `Edit -> Clear Output` in the Jupyter Lab file menu.",
      "metadata": {}
    },
    {
      "id": "6a1a8f17-d8b6-4d68-8f75-6a4a4c3a62a4",
      "cell_type": "code",
      "source": "response.text",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a254bd3-b236-410c-860d-c9987971132e",
      "cell_type": "markdown",
      "source": "And the data type of `response.text`?",
      "metadata": {}
    },
    {
      "id": "8f82398f-fd72-4580-9ba8-2c436fb00648",
      "cell_type": "code",
      "source": "type(response.text)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b1db1cab-9fe1-42d8-bec2-4fcae73077b1",
      "cell_type": "markdown",
      "source": "Not surpisingly, it's plain old text (aka a string).\n\n## A Hearty Soup\n\nWe're now ready to begin plucking the bank data out of this big blob of text using `BeautifulSoup`, which is an HTML parsing library that makes it simple to extract information from web pages. \n\n> Why soup? Perhaps because HTML is a beautiful blend of savory tags and other [\"markup\"](https://en.wikipedia.org/wiki/Markup_language). What kind of soup, you ask? Clearly it's a [goulash](https://en.wikipedia.org/wiki/Goulash).",
      "metadata": {}
    },
    {
      "id": "74b3b359-7bf4-4dc9-a62b-f710acf189e1",
      "cell_type": "code",
      "source": "import bs4\n\n# Create \"soup\" using bs4's standard, built-in HTML parser\nsoup = bs4.BeautifulSoup(response.text, 'html.parser')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1bb3d1a6-d0c9-40e9-9e94-3a80e2e60673",
      "cell_type": "markdown",
      "source": "Let's find out what we're working with.",
      "metadata": {}
    },
    {
      "id": "9a01f0e9-ab17-421e-9594-a1c8a958e4a2",
      "cell_type": "code",
      "source": "type(soup)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "95263691-51e1-4739-824c-f2242b7c0e06",
      "cell_type": "markdown",
      "source": "If we hit the BeautifulSoup docs, we'd learn that the [BeautifulSoup object](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup) provideds all sorts of nifty ways to navigate and extract content from HTML.\n\nFor example, we can select page elements by their HTML tags.",
      "metadata": {}
    },
    {
      "id": "6d759f14-782e-4b2f-90fe-c788b2b95ec3",
      "cell_type": "code",
      "source": "table = soup.find_all('table')\nlen(table)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c7181799-1f3f-4822-adb2-36ec66572bd9",
      "cell_type": "markdown",
      "source": "Looks like there's only one table on the page. And recall that our data is stored in a proper HTML table, so this seems like a promising path.\n\n## Grab the headers\n\nOur HTML table has header rows containing field names.\n\nLet's grab the row and see what we're working with.",
      "metadata": {}
    },
    {
      "id": "1961085f-779f-4da7-a32c-6f5d9868b127",
      "cell_type": "code",
      "source": "headers = soup.find_all('th')\n# Print the first column\nprint(headers[0])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "124ca6cb-7336-4a22-b612-90f74eb0b330",
      "cell_type": "markdown",
      "source": "So we have a `th` element -- which stands for *table header* --and that in turn contains a link or *anchor* (`a`) tag. \n\nIf we zoom out to the table level, the HTML tags are nested inside of each other in a structure that looks like below:\n    \n```\ntable\n  thead\n    tr\n      th\n        a\n```\n\nThe neat thing about BeautifulSoup is that it allows you to navigate, or \"walk\", the [tree structure of HTML](https://en.wikipedia.org/wiki/Document_Object_Model) by using [dot notation](../classes_and_oop/README.ipynb).",
      "metadata": {}
    },
    {
      "id": "b5bd5d46-3960-4ce3-8f70-b3d106feb42c",
      "cell_type": "code",
      "source": "# Grab the header text for the first column, \n# starting with the \"th\" tags stored in the headers variable\nheaders[0].a.text",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "062da79a-4606-4784-b8c9-05cfdd8724af",
      "cell_type": "markdown",
      "source": "Using this strategy, we can grab the text for all header cells.\n\nAlong the way, we'll do some basic cleanups on the field names. In particular, the last column called `Fund` is a big gnarly in its raw form and looks like below:\n\n```text\n 'Fund\\n\\n    Sort ascending\\n      \\n\\n'\n```\nSo we'll use the `strip` function to just grab the word `Fund`. The code as written should not affect other column headers.",
      "metadata": {}
    },
    {
      "id": "d237d7a8-bec2-4e08-a8b9-3ceedf12b295",
      "cell_type": "code",
      "source": "column_names = []\nfor th in headers:\n    col_name = th.a.text\n    clean_name = col_name.split('\\n')[0] # split on newline characters, grab base column name from beginning of list\n    column_names.append(clean_name)\ncolumn_names",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bac8494d-8b97-45d9-a718-c5d2db12d64e",
      "cell_type": "markdown",
      "source": "## Nab the data\n\nWe can repeat the above process to also extract the bank data, which is nested inside of the `tbody` tag.\n\nThe data is stored in an HTML structure that looks like below, where `tr` represents a single *table row* and `td` represents *table data*. There's a `td` element for each field in the row.\n\n```\ntable\n  tbody\n    tr\n      td\n      td\n      etc\n```",
      "metadata": {}
    },
    {
      "id": "c7e4941e-3781-450d-b6e9-2c129d73b100",
      "cell_type": "code",
      "source": "# note we use the singular \"find\" for tbody, \n# which returns the first element matching the tag name\ntbody = soup.find('tbody') \n\n# Grab all the rows inside tbody\nrows = tbody.find_all('tr')\n\n# Print number of rows (fact-check this against the count on the FDIC site)\nprint(len(rows))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e070b25c-7aac-458a-89db-033064dec725",
      "cell_type": "markdown",
      "source": "Let's inspect one of the rows to get a handle on the HTML structure.",
      "metadata": {}
    },
    {
      "id": "a39f9630-afb3-4a07-b63d-b4f13706bb5e",
      "cell_type": "code",
      "source": "rows[0]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b7ca5975-6c7c-44d8-b5b1-2246783ea385",
      "cell_type": "markdown",
      "source": "Once again, we see that `td` tags are used to store the values for individual columns. \n\nLet's collect the bank data. We'll perform some basic data clean-ups along the way and store the value from each row in a [dictionary](../python_dict_basics.ipynb).",
      "metadata": {}
    },
    {
      "id": "37aa10ff-7d8b-487a-965e-9f38dc242ca8",
      "cell_type": "code",
      "source": "all_banks = []\nfor row in rows:\n    fields = row.find_all('td')\n    field_values = [\n        fields[0].text.strip(),\n        fields[1].text.strip(),\n        fields[2].text.strip(),\n        fields[3].text.strip(),\n        fields[4].text.strip(),\n        fields[5].text.strip(),\n        fields[6].text.strip()\n    ]\n    # Mash up the headers with the field values into a dictionary\n    # - zip pairs each column header with the corresponding field in a two-element list\n    # - dict transforms the list of column/value pairs into a dictionary\n    bank_data = dict(zip(column_names, field_values))\n    all_banks.append(bank_data)\n    \n# \"Pretty Print\" a row for inspection\nfrom pprint import pprint\npprint(all_banks[0])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d56b46fd-c39f-4e0f-ac63-402b59c5e0e2",
      "cell_type": "markdown",
      "source": "How many rows of data do we have?",
      "metadata": {}
    },
    {
      "id": "5da1d1df-3a48-4a84-9824-6b2158c7e81f",
      "cell_type": "code",
      "source": "len(all_banks)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d57695a8-f62b-4c6e-9615-0afdd73bc3c3",
      "cell_type": "markdown",
      "source": "Does that number match the count on the FDIC site?",
      "metadata": {}
    },
    {
      "id": "8e78354c-31af-4746-8805-a435795d594d",
      "cell_type": "markdown",
      "source": "## Summary\n\nCongratulations! You've scraped the first page of bank data from the FDIC Failed Banks List!\n\nLet's see you flex those coding muscles on a few additional exercises.\n\n\n## Exercises\n\n### Bank Detail Page Links\n\nIf you'd like some more practice with BeautifulSoup, try modifying the code in this notebook to also extract the URL for the bank \"detail\" page. \n\nYou should get started by circling back to the HTML source code and pinpointing the URL location for banks. *Hint: The URL is stored on an [attribute](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics) of an `a` tag.*\n\nHit the BeautifulSoup docs to learn [how you can access tag attributes](https://beautiful-soup-4.readthedocs.io/en/latest/#attributes).\n\n### Get all the data\n\nWe mentioned earlier that the data on the FDIC site is \"paged\":\n\n![FDIC paging of results](../files/fdic_paging.png)\n\nIf you were to click the link for the next page, you would notice that it takes you to a new page with a slightly different URL.\n\n![FDIC page 1 results](../files/fdic_page1_url.png).\n\nYou might further notice the extra parameter on the URL: `?page=1`. \n\nAlthough this is actually the second page of results, it is perhaps confusingly called page 1. Computers, unlike humans, are fond of using `0` to identify the first item in a list, and the coders behind this site clearly let the machines decide on its pagination scheme.\n\nIn any event, the important thing to understand is that these are [predictable URLs](website_personalities.ipynb#Predictable-URLs-and-Query-Strings) -- ie URLs that you can construct predictably if you know the total number of pages. And the good news is that we do!\n\nYour mission for this exercise is to update this notebook to handle the following tasks:\n\n- Extract the total page count from the \"paging\" section at the bottom\n- Create a [*for* loop](../python_syntax_crash_course.ipynb#Loops) that steps through the full range of pages based on their number. *Hint: Check out Python's [range](https://www.w3schools.com/python/ref_func_range.asp) function for a simple approach to this problem.*\n- Extract the data from each page and store it in a list\n- **Bonus points**: Use the [csv](../python_csv.ipynb) library to export the data to an external file.\n\n## What's next?\n\nThe next step in our journey involves tackling more complex sites using a [robot to drive a web browser](drive_the_browser_robot.ipynb). Onward and upward!",
      "metadata": {}
    },
    {
      "id": "f22c374e-6c62-4cdb-9bee-ea488d2192d5",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}