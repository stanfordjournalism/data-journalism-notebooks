{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370a2fe7-649a-49ad-a6c0-f39e05f00ccf",
   "metadata": {},
   "source": [
    "# WYSIWYG Scraping\n",
    "\n",
    "There's no arguing that nerds love acronyms. And one of the best acronyms, imho, is [WYSIWYG](https://en.wikipedia.org/wiki/WYSIWYG) (pronounced wiz-ee-wig): *What You See Is What You Get*.\n",
    "\n",
    "Originally, WYSIWYG referred to software that \"[allows users to see and edit content in a form that appears as it would when displayed on an interface, webpage, slide presentation or printed document.\"](https://www.techtarget.com/whatis/definition/WYSIWYG-what-you-see-is-what-you-get)\n",
    "\n",
    "That was a sea change from having to, for example, edit HTML directly. Instead, you could hit a button on an editing panel to make some text **bold**, and the text would display in an editing window as **bold** (as opposed to displaying raw HTML such as `<strong>bold</strong>`).\n",
    "\n",
    "Why bring this up?\n",
    "\n",
    "Because it's a useful analogy for web pages. Back in the days of yore, many (perhaps most?) websites followed the WYSIWYG principle. These were simpler times, when the the content displayed on a web page closely matched the HTML in the underlying document for a page.\n",
    "\n",
    "If your web browser showed a table of data, it was quite likely that you'd find a `<table>` element somewhere in the page's HTML. \n",
    "\n",
    "This made web scrapers happy.\n",
    "\n",
    "We could write simple code to grab the page, pluck out the data, and be on our way.\n",
    "\n",
    "## A WYSIWYG example.com\n",
    "\n",
    "Those halcyon [days are waning](drive_the_browser_robot.ipynb), but you can still find websites in the wild that are coded this way.\n",
    "\n",
    "For example, compare the browser display of <http://example.com> and its underyling HTML.\n",
    "\n",
    "![example dot com display vs HTML](../files/example_dot_com_wysiwyg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926de12-4eed-45ed-af31-4ff03d3c8915",
   "metadata": {},
   "source": [
    "## Failed Banks Simple Scrape\n",
    "\n",
    "Similar to <http://example.com>, the [FDIC's list of failed banks](https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/) also embeds all of its content -- at least the data we care about -- in the HTML source code.\n",
    "\n",
    "> The site also provides the data as a downloadable CSV, which is a better alternative to scraping. But we'll ignore that for now and use the page as an opportunity to practice basic scraping techniques.\n",
    "\n",
    "How do we know that the HTML source code contains the bank data?\n",
    "\n",
    "By using the browser's developer tools to [dissect the website](dissecting_websites.ipynb).\n",
    "\n",
    "## View Page Source\n",
    "\n",
    "Step 1 involves right clicking on the page and selecting `View Page Source`.\n",
    "\n",
    "![FDIC view page source](../files/fdic_view_page_source.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6f918-2a6f-409d-a925-58fefe9f4610",
   "metadata": {},
   "source": [
    "## Compare What You See to Page Source\n",
    "\n",
    "The next step involves some basic code sleuthing: \n",
    "    \n",
    "- Review the list of banks as displayed in the browser. For example, we can see above ☝️that  `Citizens Bank` in Iowa is at the top of the list.\n",
    "- Now head over to the HTML source page you just opened (using `View Page Source`) and search the page for `Citizens Bank`.\n",
    "\n",
    "You should see that the data is embedded in the HTML.\n",
    "\n",
    "![FDIC html source code](../files/fdic_banks_html.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871cc8-5f8a-42e5-b580-a72c8954d93a",
   "metadata": {},
   "source": [
    "## Get ready to scrape\n",
    "\n",
    "We can see that the data *in our browser* (not the HTML source code) is \"paged\": I.e. you have to click through the pages to view all banks.\n",
    "\n",
    "But if you continued spelunking the source code, you'd quickly learn that all of the banks are embedded in this single HTML page.\n",
    "\n",
    "The site simply uses Javascript to present a subset of the data, likely so the page doesn't run excessively long. It's a common technique to improve a site's usability for normal page viewing and navigation of data.\n",
    "\n",
    "This simple page allows us to use two of the traditional workhorses of Python web scraping: the [requests](https://requests.readthedocs.io/en/latest/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) libraries.\n",
    "\n",
    "The former gives us the ability to fetch the source HTML for the page. The latter provides a Pythonic interface to the HTML that lets us easily extract data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fd895-aed5-4a87-ab0e-ebecc9f31abb",
   "metadata": {},
   "source": [
    "## Fetch the page\n",
    "\n",
    "> **IMPORTANT**: This section of the tutorial using `requests` only works on GitHub Codespaces or locally (if you've cloned this repo to your own machine).\n",
    "\n",
    "Let's start our scrape by requesting the page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2131f-3e25-49ec-8d93-603f25f5f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753fa48-69a7-4508-943c-ec135a99d345",
   "metadata": {},
   "source": [
    "Now inspect the `response` to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9f7c5-2cc5-4858-a8db-c01b5ed01007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the type\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc841d9-b99b-42f0-a312-0e5a8d2faaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouch, that's a long list!\n",
    "dir(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96655015-6ea9-44ca-86a2-1989c5e648ac",
   "metadata": {},
   "source": [
    "If you review the `response` object's attributes (see the [Hidden Life of Objects](../classes_and_oop/hidden_life_of_objects.ipynb) for background), you'll notice there are some potentially handy bits of data and functionality. \n",
    "\n",
    "> It's not a bad idea to review the [requests documentation on responses](https://requests.readthedocs.io/en/latest/user/quickstart/#response-content) as well. \n",
    "\n",
    "For our purposes, we're going to grab the `text` of the response, which should contain the raw HTML of the page.\n",
    "\n",
    "Prepare yourself for a LOT of HTML.\n",
    "\n",
    "> BTW, you can clear the output by selecting `Edit -> Clear Output` in the Jupyter Lab file menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a8f17-d8b6-4d68-8f75-6a4a4c3a62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a254bd3-b236-410c-860d-c9987971132e",
   "metadata": {},
   "source": [
    "And the data type of `response.text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82398f-fd72-4580-9ba8-2c436fb00648",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db1cab-9fe1-42d8-bec2-4fcae73077b1",
   "metadata": {},
   "source": [
    "Not surpisingly, it's plain old text (aka a string).\n",
    "\n",
    "## A Hearty Soup\n",
    "\n",
    "We're now ready to begin plucking the bank data out of this big blob of text using `BeautifulSoup`, which is an HTML parsing library that makes it simple to extract information from web pages. \n",
    "\n",
    "> Why soup? Perhaps because HTML is a beautiful blend of savory tags and other [\"markup\"](https://en.wikipedia.org/wiki/Markup_language). What kind of soup, you ask? Clearly it's a [goulash](https://en.wikipedia.org/wiki/Goulash)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3b359-7bf4-4dc9-a62b-f710acf189e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "# Create \"soup\" using bs4's standard, built-in HTML parser\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3d1a6-d0c9-40e9-9e94-3a80e2e60673",
   "metadata": {},
   "source": [
    "Let's find out what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01f0e9-ab17-421e-9594-a1c8a958e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95263691-51e1-4739-824c-f2242b7c0e06",
   "metadata": {},
   "source": [
    "If we hit the BeautifulSoup docs, we'd learn that the [BeautifulSoup object](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup) provideds all sorts of nifty ways to navigate and extract content from HTML.\n",
    "\n",
    "For example, we can select page elements by their HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d759f14-782e-4b2f-90fe-c788b2b95ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all('table')\n",
    "len(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7181799-1f3f-4822-adb2-36ec66572bd9",
   "metadata": {},
   "source": [
    "Looks like there's only one table on the page. And recall that our data is stored in a proper HTML table, so this seems like a promising path.\n",
    "\n",
    "## Grab the headers\n",
    "\n",
    "Our HTML table has header rows containing field names.\n",
    "\n",
    "Let's grab the row and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961085f-779f-4da7-a32c-6f5d9868b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = soup.find_all('th')\n",
    "# Print the first column\n",
    "print(headers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ca6cb-7336-4a22-b612-90f74eb0b330",
   "metadata": {},
   "source": [
    "So we have a `th` element, which in turn contains a `p` tag. \n",
    "\n",
    "Inside of the `p` tag are a pair of `span` tags.\n",
    "\n",
    "If we zoom out to the table level, the HTML tags are nested inside of each other in a structure that looks like below:\n",
    "    \n",
    "```\n",
    "table\n",
    "  thead\n",
    "    tr\n",
    "      th\n",
    "        p\n",
    "         span\n",
    "         span\n",
    "```\n",
    "\n",
    "The neat thing about BeautifulSoup is that it allows you to navigate, or \"walk\", the [tree structure of HTML](https://en.wikipedia.org/wiki/Document_Object_Model) by using [dot notation](../classes_and_oop/README.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd5d46-3960-4ce3-8f70-b3d106feb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the header text for the first column, \n",
    "# starting with the \"th\" tags stored in the headers variable\n",
    "headers[0].p.span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062da79a-4606-4784-b8c9-05cfdd8724af",
   "metadata": {},
   "source": [
    "Using this strategy, we can grab the text for all header cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237d7a8-bec2-4e08-a8b9-3ceedf12b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "for th in headers:\n",
    "    col_name = th.p.span.text\n",
    "    column_names.append(col_name)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8494d-8b97-45d9-a718-c5d2db12d64e",
   "metadata": {},
   "source": [
    "## Nab the data\n",
    "\n",
    "We can repeat the above process to also extract the bank data, which is nested inside of the `tbody` tag.\n",
    "\n",
    "The data is stored in an HTML structure that looks like below, where `tr` represents a single table row and `td` represents \"table data\". There's a `td` element for each field in the row.\n",
    "\n",
    "```\n",
    "table\n",
    "  tbody\n",
    "    tr\n",
    "      td\n",
    "      td\n",
    "      etc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4941e-3781-450d-b6e9-2c129d73b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we use the singular \"find\" for tbody, \n",
    "# which returns the first element matching the tag name\n",
    "tbody = soup.find('tbody') \n",
    "\n",
    "# Grab all the rows inside tbody\n",
    "rows = tbody.find_all('tr')\n",
    "\n",
    "# Print number of rows (fact-check this against the count on the FDIC site)\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070b25c-7aac-458a-89db-033064dec725",
   "metadata": {},
   "source": [
    "Let's inspect one of the rows to get a handle on the HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f9630-afb3-4a07-b63d-b4f13706bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca5975-6c7c-44d8-b5b1-2246783ea385",
   "metadata": {},
   "source": [
    "Once again, we see that `td` tags are used to store the values for individual columns. \n",
    "\n",
    "Let's collect the bank data. We'll perform some basic data clean-ups along the way and store the value from each row in a [dictionary](../python_dict_basics.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa10ff-7d8b-487a-965e-9f38dc242ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_banks = []\n",
    "for row in rows:\n",
    "    fields = row.find_all('td')\n",
    "    field_values = [\n",
    "        fields[0].text.strip(),\n",
    "        fields[1].text.strip(),\n",
    "        fields[2].text.strip(),\n",
    "        fields[3].text.strip(),\n",
    "        fields[4].text.strip(),\n",
    "        fields[5].text.strip(),\n",
    "        fields[6].text.strip()\n",
    "    ]\n",
    "    # Mash up the headers with the field values into a dictionary\n",
    "    # - zip creates pairs each column header with the corresponding field in a two-element list\n",
    "    # - dict transforms the list of column/value pairs into a dictionary\n",
    "    bank_data = dict(zip(column_names, field_values))\n",
    "    all_banks.append(bank_data)\n",
    "    \n",
    "# Print a row for inspection\n",
    "print(all_banks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b46fd-c39f-4e0f-ac63-402b59c5e0e2",
   "metadata": {},
   "source": [
    "How many rows of data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1d1df-3a48-4a84-9824-6b2158c7e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_banks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57695a8-f62b-4c6e-9615-0afdd73bc3c3",
   "metadata": {},
   "source": [
    "Does that number match the count on the FDIC site? And in their download CSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78354c-31af-4746-8805-a435795d594d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've scraped all the bank data from the FDIC Failed Banks List.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "If you'd like some more practice with BeautifulSoup, try modifying the code in this notebook to also extract the URL for the bank \"detail\" page. \n",
    "\n",
    "You should get started by circling back to the HTML source code and pinpointing the URL location for banks. *Hint: The URL is stored on an [attribute](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics) of an `a` tag.*\n",
    "\n",
    "Hit the BeautifulSoup docs to learn [how you can access tag attributes](https://beautiful-soup-4.readthedocs.io/en/latest/#attributes).\n",
    "\n",
    "Bonus points: Use the [csv](../python_csv.ipynb) library to export the data to an external file.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next step in our journey involves tackling more complex sites using a [robot to drive a web browser](drive_the_browser_robot.ipynb). Onward and upward!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
